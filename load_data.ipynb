{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load variables\n",
    "import numpy as np\n",
    "import librosa\n",
    "from librosa.display import specshow\n",
    "from librosa.util import softmask\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import sys, os\n",
    "import types \n",
    "%matplotlib inline\n",
    "\n",
    "# load tensorflow and keras \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, Reshape, Concatenate\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D, ReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "import tensorflow.keras.backend as K\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables \n",
    "path = '/home/jangryga/source-separation/MSD100/'  # your path to the MSD100\n",
    "rate = 44100                                       # sampling rate at which songs are loaded\n",
    "hop_length = 256                                   # hop_length for the fft\n",
    "n_fft = 1024                                       # window length of the fft\n",
    "list_titles = os.listdir(os.path.join(os.path.join(path,'Mixtures'),'Dev'))        # train file list with titels \n",
    "list_titles_test = os.listdir(os.path.join(os.path.join(path,'Mixtures'),'Test'))  # test file list with titles \n",
    "time_len = 30                                      # time length of a batch\n",
    "n_frames = 32                                      # number of examples in a single batch\n",
    "overlap = 15                                       # overlap between examples\n",
    "normalization = np.sqrt(1024)                      # sqrt of the frame size\n",
    "scale_mag = 0.3                                    # further normalization\n",
    "batch = np.empty(shape=(n_frames, int(n_fft/2+1), time_len))    # init empty batch\n",
    "a,b,c,d,e = (0,0,0,0,0)                            # init variables a,b,c,d,e\n",
    "\n",
    "# perameters for the NN \n",
    "steps_per_epoch = 4810                             # total num of batches\n",
    "n_epochs=1                                         # number of training epochs \n",
    "number = steps_per_epoch * n_epochs                # total number of batches to load for training\n",
    "max_queue = 200                                    # how many batches to load into memory at once\n",
    "multiprocessing = True                            \n",
    "validation_steps = 467                             # number of validation batches \n",
    "n_workers=3\n",
    "epsilon=1e-8                                       # ... \n",
    "alpha=0.001                                        # ...\n",
    "beta=0.01                                          # ...\n",
    "beta_voc=0.03                                      # ...\n",
    "rand_num = np.random.uniform(size=(32,513,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator to load tracks for training \n",
    "\n",
    "def file_gen(iterable):\n",
    "    \n",
    "    saved = []   # list that generator iterates over after first epoch \n",
    "    \n",
    "    for element in range(len(iterable)):\n",
    "        \n",
    "        # load mixture tracks\n",
    "        path_ = os.path.join(os.path.join(path,'Mixtures'), 'Dev')\n",
    "        os.chdir(os.path.join(path_,iterable[element]))\n",
    "        y, _ = librosa.load('mixture.wav', sr=rate)\n",
    "\n",
    "        \n",
    "        # load vocal tracks & possible other tracks \n",
    "        path_vox = os.path.join(os.path.join(path,'Sources'), 'Dev')\n",
    "        os.chdir(os.path.join(path_vox,iterable[element]))\n",
    "        x, _ = librosa.load('vocals.wav', sr=rate)\n",
    "        w, _ = librosa.load('bass.wav', sr=rate)\n",
    "        z, _ = librosa.load('drums.wav', sr=rate)\n",
    "        h, _ = librosa.load('other.wav', sr=rate)\n",
    "        yield y, x, w, z, h\n",
    "        \n",
    "        # append elements to the saved list\n",
    "        saved.append(list_titles[element])\n",
    "        \n",
    "        # generator iterates indefinitely over the saved list\n",
    "    while saved:\n",
    "        for element in saved:\n",
    "            path_ = os.path.join(os.path.join(path,'Mixtures'), 'Dev')\n",
    "            os.chdir(os.path.join(path_,element))\n",
    "            y, _ = librosa.load('mixture.wav', sr=rate)\n",
    "\n",
    "            path_vox = os.path.join(os.path.join(path,'Sources'), 'Dev')\n",
    "            os.chdir(os.path.join(path_vox,element))\n",
    "            x, _ = librosa.load('vocals.wav', sr=rate)\n",
    "            w, _ = librosa.load('bass.wav', sr=rate)\n",
    "            z, _ = librosa.load('drums.wav', sr=rate)\n",
    "            h, _ = librosa.load('other.wav', sr=rate)\n",
    "        \n",
    "            yield y, x, w, z, h\n",
    "\"\"\"\n",
    "generator to load files for testing \n",
    "to do:   code to be optimised to avoid defining seperate generators  \n",
    "         e.g., generator could take test/train as a condition variable \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def file_gen_test(iterable):\n",
    "    \n",
    "    saved = []\n",
    "    \n",
    "    for element in range(len(iterable)):\n",
    "        path_ = os.path.join(os.path.join(path,'Mixtures'), 'Test')\n",
    "        os.chdir(os.path.join(path_,iterable[element]))\n",
    "        y, _ = librosa.load('mixture.wav', sr=rate)\n",
    "        \n",
    "        path_vox = os.path.join(os.path.join(path,'Sources'), 'Test')\n",
    "        os.chdir(os.path.join(path_vox,iterable[element]))\n",
    "        x, _ = librosa.load('vocals.wav', sr=rate)\n",
    "        w, _ = librosa.load('bass.wav', sr=rate)\n",
    "        z, _ = librosa.load('drums.wav', sr=rate)\n",
    "        h, _ = librosa.load('other.wav', sr=rate)\n",
    "        yield y, x, w, z, h\n",
    "        \n",
    "        saved.append(list_titles_test[element])\n",
    "        \n",
    "    while saved:\n",
    "        for element in saved:\n",
    "            path_ = os.path.join(os.path.join(path,'Mixtures'), 'Test')\n",
    "            os.chdir(os.path.join(path_,element))\n",
    "            y, _ = librosa.load('mixture.wav', sr=rate)\n",
    "\n",
    "            path_vox = os.path.join(os.path.join(path,'Sources'), 'Test')\n",
    "            os.chdir(os.path.join(path_vox,element))\n",
    "            x, _ = librosa.load('vocals.wav', sr=rate)\n",
    "            w, _ = librosa.load('bass.wav', sr=rate)\n",
    "            z, _ = librosa.load('drums.wav', sr=rate)\n",
    "            h, _ = librosa.load('other.wav', sr=rate)\n",
    "            yield y, x, w, z, h\n",
    "            \n",
    "# initialize generators             \n",
    "gen_file = file_gen(list_titles)  \n",
    "gen_file_test = file_gen_test(list_titles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(b):\n",
    "    # compute a spectrogram of b\n",
    "    return librosa.stft(b, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "def mag_phase(b):\n",
    "    # return matude and phase arrays\n",
    "    a, b = librosa.magphase(b)\n",
    "    return a, b\n",
    "\n",
    "def softmask(x, x_ref):\n",
    "    # create softmask for the x and x_ref \n",
    "    return librosa.util.softmask(x,x_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generator that takes files and loads   \n",
    "to do: \n",
    "- catch when initialised with parameters outside of constraints\n",
    "- cobmibe to into a single generator+wrapper for testing and training\n",
    "- return the phase as option \n",
    "\"\"\"\n",
    "def return_batch(num, a, b, c, d, e):\n",
    "    \"\"\"\n",
    "    a - mix magnitude spectrogram\n",
    "    b - vocal magnitude spectrogram \n",
    "    num - number of batches - after it goes to 0, load new files and update itself -> wrapper function \n",
    "    \"\"\"\n",
    "    memory = num\n",
    "    batch_a = np.empty((32,513,30))\n",
    "    batch_b = np.empty((32,513,30))\n",
    "    batch_c = np.empty((32,513,30))\n",
    "    batch_d = np.empty((32,513,30))\n",
    "    batch_e = np.empty((32,513,30))\n",
    "    \n",
    "    index = 0\n",
    "    batch_len = (n_frames-1)*(time_len-overlap) + time_len\n",
    "\n",
    "    # transform tracks into \n",
    "    while memory > 0:\n",
    "        for n in range(32):\n",
    "            batch_a[n] = a[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_b[n] = b[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_c[n] = c[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_d[n] = d[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_e[n] = e[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]\n",
    "        \n",
    "        yield batch_a, batch_b, batch_c, batch_d, batch_e\n",
    "        \n",
    "        #update memory and index\n",
    "        memory -= 1\n",
    "        index +=1\n",
    "        \n",
    "# function that lets the inside generator iterate indefinitely\n",
    "def wrapper():\n",
    "    global batch_tr\n",
    "    try:\n",
    "        ok = next(batch_tr)\n",
    "        return ok\n",
    "    except StopIteration:\n",
    "        \n",
    "        # load new file using gen_file generator\n",
    "        a, b, c, d, e = next(gen_file)\n",
    "        \n",
    "        # transform audio data into magnitude spectrograms, forgo the phase \n",
    "        a, _ = mag_phase(spectrogram(a))\n",
    "        b, _ = mag_phase(spectrogram(b))\n",
    "        c, _ = mag_phase(spectrogram(c))\n",
    "        d, _ = mag_phase(spectrogram(d))\n",
    "        e, _ = mag_phase(spectrogram(e))\n",
    "        # memory - number of batches in the file \n",
    "        memory = a.shape[1]/((n_frames-1)*(time_len-overlap) + time_len)\n",
    "        batch = return_batch(memory, a, b, c, d, e)\n",
    "        ok = next(batch)\n",
    "        return ok\n",
    "    \n",
    "# outside generator for fit_model\n",
    "def gen_train(num_batch):\n",
    "    while num_batch > 0:\n",
    "        \n",
    "        # call wrapper function that can iterate indefinitely\n",
    "        x,y,z,w,h = wrapper()\n",
    "        \n",
    "        # reshape the file for the output\n",
    "        x = np.reshape(x,(32,513,30,1))\n",
    "        y = np.reshape(y,(32,513,30,1))\n",
    "        z = np.reshape(z,(32,513,30,1))\n",
    "        w = np.reshape(w,(32,513,30,1))\n",
    "        h = np.reshape(h,(32,513,30,1))\n",
    "    \n",
    "        y = np.append(y, z, axis=3)\n",
    "        y = np.append(y, w, axis=3)\n",
    "        y = np.append(y, h, axis=3)\n",
    "        \n",
    "        #normalize \n",
    "        y = y / normalization \n",
    "        y = scale_mag*y.astype(np.float32)\n",
    "    \n",
    "        x = x / normalization\n",
    "        x = scale_mag*x.astype(np.float32)\n",
    "        \n",
    "        yield x,y \n",
    "        \n",
    "    \n",
    "def return_batch_test(num, a, b, c, d, e):\n",
    "    \n",
    "    memory_test = num\n",
    "    batch_a = np.empty((32,513,30))\n",
    "    batch_b = np.empty((32,513,30))\n",
    "    batch_c = np.empty((32,513,30))\n",
    "    batch_d = np.empty((32,513,30))\n",
    "    batch_e = np.empty((32,513,30))\n",
    "    index_test = 0\n",
    "    batch_len = (n_frames-1)*(time_len-overlap) + time_len\n",
    "\n",
    "\n",
    "    while memory_test > 0:\n",
    "        for n in range(32):\n",
    "            batch_a[n] = a[:,index_test*batch_len + n*(time_len-overlap): index_test*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_b[n] = b[:,index_test*batch_len + n*(time_len-overlap): index_test*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_c[n] = c[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_d[n] = d[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]\n",
    "        for n in range(32):\n",
    "            batch_e[n] = e[:,index*batch_len + n*(time_len-overlap): index*batch_len + n*(time_len-overlap)+time_len]        \n",
    "        \n",
    "        yield batch_a, batch_b, batch_c, batch_d, batch_e\n",
    "        memory_test -= 1\n",
    "        index_test +=1\n",
    "        \n",
    "\n",
    "def wrapper_test():\n",
    "    global batch_test\n",
    "    try:\n",
    "        ok = next(batch_test)\n",
    "        return ok\n",
    "    except StopIteration:\n",
    "        a, b, c, d, e = next(gen_file_test)\n",
    "        a, _ = mag_phase(spectrogram(a))\n",
    "        b, _ = mag_phase(spectrogram(b))\n",
    "        c, _ = mag_phase(spectrogram(c))\n",
    "        d, _ = mag_phase(spectrogram(d))\n",
    "        e, _ = mag_phase(spectrogram(e))\n",
    "        memory = a.shape[1]/((n_frames-1)*(time_len-overlap) + time_len)\n",
    "        batch_test = return_batch_test(memory, a, b, c, d, e)\n",
    "        ok = next(batch_test)\n",
    "        return ok\n",
    "    \n",
    "def gen_test(num_batch):\n",
    "    while num_batch > 0:\n",
    "        # call wrapper function that can iterate indefinitely\n",
    "        x,y,z,w,h = wrapper_test()\n",
    "        \n",
    "        # reshape the file for the output\n",
    "        x = np.reshape(x,(32,513,30,1))\n",
    "        y = np.reshape(y,(32,513,30,1))\n",
    "        z = np.reshape(z,(32,513,30,1))\n",
    "        w = np.reshape(w,(32,513,30,1))\n",
    "        h = np.reshape(h,(32,513,30,1))\n",
    "    \n",
    "        y = np.append(y, z, axis=3)\n",
    "        y = np.append(y, w, axis=3)\n",
    "        y = np.append(y, h, axis=3)\n",
    "        \n",
    "        #normalize \n",
    "        y = y / normalization \n",
    "        y = scale_mag*y.astype(np.float32)\n",
    "    \n",
    "        x = x / normalization\n",
    "        x = scale_mag*x.astype(np.float32)\n",
    "        \n",
    "        yield x,y  \n",
    "\n",
    "# initialize inside generators - 0 parameter prompts the except route that loads new data\n",
    "batch_test = return_batch_test(0, a, b, c, d, e)\n",
    "batch_tr = return_batch(0, a, b, c, d, e)\n",
    "\n",
    "# initialize outside generators for fit_model \n",
    "gen_ts = gen_test(number)\n",
    "gen_tr = gen_train(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(513,30,1),batch_size=32)\n",
    "layer_conv1 = Conv2D(filters=50, kernel_size=(513,1), padding='valid')(inp)\n",
    "layer_conv2 = Conv2D(filters=50, kernel_size=(1,15), padding='valid')(layer_conv1)\n",
    "layer_flat = Flatten()(layer_conv2)\n",
    "layer_dense = Dense(units=128, activation='relu')(layer_flat)\n",
    "\n",
    "b1 = Dense(units=int(layer_flat.shape[1]), activation='relu')(layer_dense)\n",
    "b1 = Reshape(target_shape=(int(layer_conv2.shape[1]),int(layer_conv2.shape[2]),int(layer_conv2.shape[3])))(b1)\n",
    "b1 = Conv2DTranspose(filters=50, kernel_size=(1,15), padding='valid')(b1)\n",
    "b1 = Conv2DTranspose(filters=1, kernel_size=(513,1), padding='valid')(b1)\n",
    "\n",
    "b2 = Dense(units=int(layer_flat.shape[1]), activation='relu')(layer_dense)\n",
    "b2 = Reshape(target_shape=(int(layer_conv2.shape[1]),int(layer_conv2.shape[2]),int(layer_conv2.shape[3])))(b2)\n",
    "b2 = Conv2DTranspose(filters=50, kernel_size=(1,15), padding='valid')(b2)\n",
    "b2 = Conv2DTranspose(filters=1, kernel_size=(513,1), padding='valid')(b2)\n",
    "\n",
    "\n",
    "b3 = Dense(units=int(layer_flat.shape[1]), activation='relu')(layer_dense)\n",
    "b3 = Reshape(target_shape=(int(layer_conv2.shape[1]),int(layer_conv2.shape[2]),int(layer_conv2.shape[3])))(b3)\n",
    "b3 = Conv2DTranspose(filters=50, kernel_size=(1,15), padding='valid')(b3)\n",
    "b3 = Conv2DTranspose(filters=1, kernel_size=(513,1), padding='valid')(b3)\n",
    "\n",
    "\n",
    "b4 = Dense(units=int(layer_flat.shape[1]), activation='relu')(layer_dense)\n",
    "b4 = Reshape(target_shape=(int(layer_conv2.shape[1]),int(layer_conv2.shape[2]),int(layer_conv2.shape[3])))(b4)\n",
    "b4 = Conv2DTranspose(filters=50, kernel_size=(1,15), padding='valid')(b4)\n",
    "b4 = Conv2DTranspose(filters=1, kernel_size=(513,1), padding='valid')(b4)\n",
    "\n",
    "\n",
    "out = Concatenate(axis=3)([b1,b2,b3,b4])\n",
    "out = ReLU()(out)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    \n",
    "    global alpha, beta, beta_voc, rand_num\n",
    "    \n",
    "        \n",
    "    voc =  y_pred[:,:,:,0:1] + epsilon * rand_num\n",
    "    bass = y_pred[:,:,:,1:2] + epsilon * rand_num\n",
    "    dru = y_pred[:,:,:,2:3] + epsilon * rand_num\n",
    "    oth = y_pred[:,:,:,3:4] + epsilon * rand_num\n",
    "        \n",
    "    mask_vox = voc/(voc+bass+dru+oth)\n",
    "    mask_bass = bass/(voc+bass+dru+oth)\n",
    "    mask_drums = dru/(voc+bass+dru+oth)\n",
    "    mask_oth = oth/(voc+bass+dru+oth)\n",
    "\n",
    "    vocals = mask_vox * inp\n",
    "    bass = mask_bass * inp\n",
    "    drums = mask_drums * inp\n",
    "    other = mask_oth * inp\n",
    "    \n",
    "    train_loss_vocals = mean_squared_error(y_true=y_true[:,:,:,0:1],y_pred=vocals)\n",
    "    alpha_component = alpha*mean_squared_error(y_true=y_true[:,:,:,1:2],y_pred=vocals)\n",
    "    alpha_component += alpha*mean_squared_error(y_true=y_true[:,:,:,2:3],y_pred=vocals)\n",
    "    train_loss_recon_neg_voc = beta_voc*mean_squared_error(y_true=y_true[:,:,:,3:4],y_pred=vocals)\n",
    "    \n",
    "    train_loss_bass = mean_squared_error(y_true=y_true[:,:,:,1:2],y_pred=bass)\n",
    "    alpha_component += alpha*mean_squared_error(y_true=y_true[:,:,:,0:1],y_pred=bass)\n",
    "    alpha_component += alpha*mean_squared_error(y_true=y_true[:,:,:,2:3],y_pred=bass)\n",
    "    train_loss_recon_neg = beta*mean_squared_error(y_true=y_true[:,:,:,3:4],y_pred=bass)\n",
    "    \n",
    "    train_loss_drums = mean_squared_error(y_true=y_true[:,:,:,2:3],y_pred=drums)\n",
    "    alpha_component += alpha*mean_squared_error(y_true=y_true[:,:,:,0:1],y_pred=drums)\n",
    "    alpha_component += alpha*mean_squared_error(y_true=y_true[:,:,:,1:2],y_pred=drums)\n",
    "    train_loss_recon_neg += beta*mean_squared_error(y_true=y_true[:,:,:,3:4],y_pred=drums)\n",
    "    \n",
    "    vocals_error= K.sum(train_loss_vocals)\n",
    "    drums_error= K.sum(train_loss_drums)\n",
    "    bass_error= K.sum(train_loss_bass)\n",
    "    negative_error= K.sum(train_loss_recon_neg)\n",
    "    negative_error_voc= K.sum(train_loss_recon_neg_voc)\n",
    "    alpha_component= K.sum(alpha_component)\n",
    "\n",
    "    loss=K.abs(vocals_error+drums_error+bass_error-negative_error-alpha_component-negative_error_voc)\n",
    "    \n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "   2/4810 [..............................] - ETA: 179:03:18 - loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-3:\n",
      "Process PoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 680, in next_sample\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 680, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-9-1158559417dc>\", line 71, in gen_train\n",
      "  File \"<ipython-input-9-1158559417dc>\", line 71, in gen_train\n",
      "    x,y,z,w,h = wrapper()\n",
      "  File \"<ipython-input-9-1158559417dc>\", line 52, in wrapper\n",
      "Process PoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 680, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-9-1158559417dc>\", line 71, in gen_train\n",
      "    x,y,z,w,h = wrapper()\n",
      "  File \"<ipython-input-9-1158559417dc>\", line 52, in wrapper\n",
      "    x,y,z,w,h = wrapper()\n",
      "  File \"<ipython-input-9-1158559417dc>\", line 52, in wrapper\n",
      "    a, b, c, d, e = next(gen_file)\n",
      "  File \"<ipython-input-7-07980c7bc4f0>\", line 19, in file_gen\n",
      "    a, b, c, d, e = next(gen_file)\n",
      "  File \"<ipython-input-7-07980c7bc4f0>\", line 19, in file_gen\n",
      "    w, _ = librosa.load('bass.wav', sr=rate)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/librosa/core/audio.py\", line 133, in load\n",
      "    for frame in input_file:\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/audioread/rawread.py\", line 130, in read_data\n",
      "    data = self._file.readframes(block_samples)\n",
      "  File \"/usr/lib/python2.7/wave.py\", line 256, in readframes\n",
      "    data = self._data_chunk.read(nframes * self._framesize)\n",
      "  File \"/usr/lib/python2.7/chunk.py\", line 136, in read\n",
      "    data = self.file.read(size)\n",
      "  File \"/usr/lib/python2.7/chunk.py\", line 136, in read\n",
      "    data = self.file.read(size)\n",
      "KeyboardInterrupt\n",
      "    a, b, c, d, e = next(gen_file)\n",
      "  File \"<ipython-input-7-07980c7bc4f0>\", line 19, in file_gen\n",
      "    w, _ = librosa.load('bass.wav', sr=rate)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/librosa/core/audio.py\", line 133, in load\n",
      "    for frame in input_file:\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/audioread/rawread.py\", line 130, in read_data\n",
      "    data = self._file.readframes(block_samples)\n",
      "  File \"/usr/lib/python2.7/wave.py\", line 256, in readframes\n",
      "    data = self._data_chunk.read(nframes * self._framesize)\n",
      "  File \"/usr/lib/python2.7/chunk.py\", line 136, in read\n",
      "    data = self.file.read(size)\n",
      "  File \"/usr/lib/python2.7/chunk.py\", line 136, in read\n",
      "    data = self.file.read(size)\n",
      "KeyboardInterrupt\n",
      "    w, _ = librosa.load('bass.wav', sr=rate)\n",
      "Process PoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "Process PoolWorker-5:\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 724, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 718, in pool_fn\n",
      "    initargs=(seqs, self.random_seed))\n",
      "  File \"/usr/lib/python2.7/multiprocessing/__init__.py\", line 232, in Pool\n",
      "    return Pool(processes, initializer, initargs, maxtasksperchild)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 159, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 223, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 130, in start\n",
      "    self._popen = Popen(self)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/forking.py\", line 126, in __init__\n",
      "    code = process_obj._bootstrap()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 276, in _bootstrap\n",
      "    traceback.print_exc()\n",
      "  File \"/usr/lib/python2.7/traceback.py\", line 233, in print_exc\n",
      "    print_exception(etype, value, tb, limit, file)\n",
      "  File \"/usr/lib/python2.7/traceback.py\", line 125, in print_exception\n",
      "    print_tb(tb, limit, file)\n",
      "  File \"/usr/lib/python2.7/traceback.py\", line 70, in print_tb\n",
      "    if line: _print(file, '    ' + line.strip())\n",
      "  File \"/usr/lib/python2.7/traceback.py\", line 13, in _print\n",
      "    file.write(str+terminator)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.py\", line 406, in write\n",
      "    self.flush()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.py\", line 354, in flush\n",
      "    self._flush()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.py\", line 380, in _flush\n",
      "    parent=self.parent_header, ident=self.topic)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_client/session.py\", line 748, in send\n",
      "    stream.send_multipart(to_send, copy=copy)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.py\", line 212, in send_multipart\n",
      "    self.schedule(lambda : self._really_send(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.py\", line 212, in <lambda>\n",
      "    self.schedule(lambda : self._really_send(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.py\", line 228, in _really_send\n",
      "    ctx.term()\n",
      "  File \"zmq/backend/cython/context.pyx\", line 136, in zmq.backend.cython.context.Context.term\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "    PyErr_CheckSignals()\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train and compile \n",
    "model.compile(loss=loss_func, optimizer=\"adam\")\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='/home/jangryga/source-separation/checkpoints/weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit_generator(generator=gen_tr, steps_per_epoch=4810, epochs=n_epochs, max_queue_size=max_queue, \n",
    "                    use_multiprocessing=multiprocessing, validation_data=gen_ts,\n",
    "                    validation_steps=467, workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
